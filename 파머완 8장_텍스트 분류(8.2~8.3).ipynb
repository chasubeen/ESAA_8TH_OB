{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chasubeen/ESAA_8th_OB/blob/Week_5/%ED%8C%8C%EB%A8%B8%EC%99%84%208%EC%9E%A5_%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%B6%84%EB%A5%98(8.2~8.3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUW9Y1TG9JUr"
      },
      "source": [
        "# **8-2. 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU0q1bWa9JUv"
      },
      "source": [
        "### **Text Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUMAwm_o9JUv",
        "outputId": "0be6b739-48c1-4ce7-a3f7-783febbc2809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt') # 마침표, 개행 문자 등의 데이터 세트 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 문장 토큰화\n",
        "\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "               You can see it out your window or on your television. \\\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\n",
        "sentences = sent_tokenize(text = text_sample) # 토큰화\n",
        "print(type(sentences),len(sentences))\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "WnLNt4N_98_T",
        "outputId": "06f8c4a2-766e-4fa3-c177-95c46335b147",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VBoOEXiW9JUx",
        "outputId": "27bcf583-ef32-405d-ffc4-e501a5a24369",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ],
      "source": [
        "### 단어 토큰화\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w-s8EJ2g9JUx",
        "outputId": "1e7b023b-9a9c-4040-f71a-8ff649f7484d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ],
      "source": [
        "### 문장 토큰화 + 단어 토큰화\n",
        "\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "## 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하는 함수\n",
        "def tokenize_text(text):\n",
        "  # 문장 토큰화\n",
        "  sentences = sent_tokenize(text)\n",
        "  # 분리된 문장별로 단어 토큰화\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokens\n",
        "\n",
        "# 여러 문장들에 대해 문장별 단어 토큰화 수행\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens),len(word_tokens))\n",
        "print(word_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT80I_Mp9JUy"
      },
      "source": [
        "### **Stopwords 제거**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "60e82KOM9JUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed35b7c7-18d0-498e-eec4-c47b3472bad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') # stop word 목록 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ny_gj4eG9JUy",
        "outputId": "63d85472-5aeb-4ae2-e375-0ea8bc92b929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 stop words 갯수: 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ]
        }
      ],
      "source": [
        "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "si4ZRJGl9JUz",
        "outputId": "36fdb417-e3a4-4459-abf5-881fd124050b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "\n",
        "## stop word 제거\n",
        "for sentence in word_tokens:\n",
        "  filtered_words = []\n",
        "  # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거\n",
        "  for word in sentence:\n",
        "    # 소문자로 모두 변환\n",
        "    word = word.lower()\n",
        "    # tokenize된 개별 word가 stop word가 아니면 word_tokens에 추가\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "  # 전체 단어 토큰 목록에 추가\n",
        "  all_tokens.append(filtered_words)\n",
        "\n",
        "print(all_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_i9J3s49JUz"
      },
      "source": [
        "### **Stemming과 Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D7dx2Qz79JU0",
        "outputId": "c50a8441-9fc8-4bba-80a6-7389ff1d8f9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ],
      "source": [
        "## Stemming\n",
        "\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W0oWpO_x9JU0",
        "outputId": "1d312498-e325-48b3-998d-aa6635ccd6d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ]
        }
      ],
      "source": [
        "## Lemmatization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
        "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJTerVnB9JU0"
      },
      "source": [
        "# **8-3. Bag of Words – BOW**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuagMyl79JU0"
      },
      "source": [
        "### **희소 행렬 - COO 형식**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PcwoWrIx9JU0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "dense = np.array([ [ 3, 0, 1 ], [0, 2, 0 ] ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "N1svt_o69JU1"
      },
      "outputs": [],
      "source": [
        "from scipy import sparse\n",
        "\n",
        "# 0이 아닌 데이터 추출\n",
        "data = np.array([3,1,2])\n",
        "\n",
        "# 행 위치와 열 위치를 각각 array로 생성\n",
        "row_pos = np.array([0,0,1])\n",
        "col_pos = np.array([0,2,1])\n",
        "\n",
        "# COO 형식으로 희소 행렬 생성\n",
        "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "loe34xan9JU1",
        "outputId": "ce659c1f-0cbe-4366-ffbb-9af0ba1b0a6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# 희소 행렬 -> 밀집 행렬\n",
        "\n",
        "sparse_coo.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNSHgEqt9JU1"
      },
      "source": [
        "### **희소 행렬 – CSR 형식**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6fh32oy-9JU1",
        "outputId": "fd002b5d-a849-47c3-d47f-7765ac8196a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "from scipy import sparse\n",
        "\n",
        "dense2 = np.array([[0,0,1,0,0,5],\n",
        "             [1,4,0,3,2,5],\n",
        "             [0,6,0,3,0,0],\n",
        "             [2,0,0,0,0,0],\n",
        "             [0,0,0,7,0,8],\n",
        "             [1,0,0,0,0,0]])\n",
        "\n",
        "# 0이 아닌 데이터 추출\n",
        "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
        "\n",
        "# 행 위치와 열 위치를 각각 array로 생성\n",
        "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
        "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
        "\n",
        "# COO 형식으로 변환\n",
        "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
        "\n",
        "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
        "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
        "\n",
        "# CSR 형식으로 변환\n",
        "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
        "\n",
        "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
        "print(sparse_coo.toarray())\n",
        "\n",
        "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
        "print(sparse_csr.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PP4C5iSi9JU1"
      },
      "outputs": [],
      "source": [
        "dense3 = np.array([[0,0,1,0,0,5],\n",
        "             [1,4,0,3,2,5],\n",
        "             [0,6,0,3,0,0],\n",
        "             [2,0,0,0,0,0],\n",
        "             [0,0,0,7,0,8],\n",
        "             [1,0,0,0,0,0]])\n",
        "\n",
        "# 밀집 행렬을 자동으로 희소 행렬로 바꿔줌\n",
        "coo = sparse.coo_matrix(dense3)\n",
        "csr = sparse.csr_matrix(dense3)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}